{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Galaxy Zoo CNN\n",
    "\n",
    "This notebook outlines a basic Convolutional Neural Network (CNN) for classifying galaxy images from the Galaxy Zoo project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Add any other libraries you might need, e.g., for data loading or specific layers\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Preprocess Data\n",
    "\n",
    "This section is a placeholder. You'll need to replace it with actual code to load and preprocess your Galaxy Zoo dataset.\n",
    "\n",
    "Key steps typically include:\n",
    "- Loading images and their corresponding labels.\n",
    "- Resizing images to a consistent input shape for the CNN.\n",
    "- Normalizing pixel values (e.g., scaling to [0, 1]).\n",
    "- Splitting data into training, validation, and test sets.\n",
    "- One-hot encoding labels if you have multiple classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for data loading and preprocessing\n",
    "# Ensure your data is in the format: (num_samples, height, width, channels)\n",
    "# Example:\n",
    "# (x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data() \n",
    "\n",
    "img_height = 128  # Example: Adjust based on your dataset and experimentation\n",
    "img_width = 128   # Example: Adjust based on your dataset and experimentation\n",
    "num_classes = 10  # Example: Adjust to the number of galaxy types you're classifying\n",
    "\n",
    "# --- Replace with your actual data loading below --- \n",
    "# x_train = ... # Training images (NumPy array)\n",
    "# y_train = ... # Training labels (NumPy array)\n",
    "# x_val = ...   # Validation images (NumPy array)\n",
    "# y_val = ...   # Validation labels (NumPy array)\n",
    "# x_test = ...  # Test images (NumPy array)\n",
    "# y_test = ...  # Test labels (NumPy array)\n",
    "\n",
    "# print(f\"x_train shape: {x_train.shape}\")\n",
    "# print(f\"y_train shape: {y_train.shape}\")\n",
    "\n",
    "# # Normalize pixel values to be between 0 and 1\n",
    "# x_train = x_train.astype('float32') / 255.0\n",
    "# x_val = x_val.astype('float32') / 255.0\n",
    "# x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# # Convert class vectors to binary class matrices (one-hot encoding)\n",
    "# y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "# y_val = keras.utils.to_categorical(y_val, num_classes)\n",
    "# y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define the CNN Model Architecture\n",
    "\n",
    "This is a basic example. You might need to adjust the number of layers, filters, kernel sizes, pooling, and dropout rates based on your specific dataset and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        # Input layer - specify input_shape for the first layer\n",
    "        layers.Input(shape=(img_height, img_width, 3)), # Assuming RGB images (3 channels)\n",
    "        \n",
    "        # Convolutional Block 1\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        # Convolutional Block 2\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        # Convolutional Block 3\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        # Flattening the feature maps\n",
    "        layers.Flatten(),\n",
    "        \n",
    "        # Dense (fully connected) layers\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.5), # Dropout for regularization\n",
    "        layers.Dense(num_classes, activation='softmax') # Output layer - softmax for multi-class classification\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compile the Model\n",
    "\n",
    "Configure the learning process. This involves choosing:\n",
    "- **Optimizer:** Algorithm to update weights (e.g., Adam, SGD).\n",
    "- **Loss Function:** Measures how well the model is doing (e.g., `categorical_crossentropy` for multi-class classification).\n",
    "- **Metrics:** Used to monitor training and testing steps (e.g., `accuracy`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy', # Use 'binary_crossentropy' for binary classification\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train the Model\n",
    "\n",
    "This is where the model learns from the training data. You'll need to provide your training and validation data.\n",
    "\n",
    "Key parameters:\n",
    "- `epochs`: Number of times the model will iterate over the entire training dataset.\n",
    "- `batch_size`: Number of samples processed before the model's internal parameters are updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for model training - uncomment and adapt when data is loaded\n",
    "# Make sure x_train, y_train, x_val, y_val are defined in Section 2.\n",
    "\n",
    "# epochs = 25 # Example: Adjust as needed\n",
    "# batch_size = 32 # Example: Adjust as needed\n",
    "\n",
    "# history = model.fit(\n",
    "#     x_train, y_train,\n",
    "#     epochs=epochs,\n",
    "#     batch_size=batch_size,\n",
    "#     validation_data=(x_val, y_val) # Provide validation data to monitor performance\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Training History (Optional but Recommended)\n",
    "\n",
    "Visualizing training and validation loss/accuracy can help diagnose overfitting or underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_history(history_object):\n",
    "#     acc = history_object.history['accuracy']\n",
    "#     val_acc = history_object.history['val_accuracy']\n",
    "#     loss = history_object.history['loss']\n",
    "#     val_loss = history_object.history['val_loss']\n",
    "#     epochs_range = range(len(acc))\n",
    "\n",
    "#     plt.figure(figsize=(12, 4))\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "#     plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "#     plt.legend(loc='lower right')\n",
    "#     plt.title('Training and Validation Accuracy')\n",
    "\n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     plt.plot(epochs_range, loss, label='Training Loss')\n",
    "#     plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "#     plt.legend(loc='upper right')\n",
    "#     plt.title('Training and Validation Loss')\n",
    "#     plt.show()\n",
    "\n",
    "# # Call this function after training (if history object is available)\n",
    "# # if 'history' in locals():\n",
    "# #    plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate the Model\n",
    "\n",
    "Assess the model's performance on unseen test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for model evaluation - uncomment and adapt when data is loaded\n",
    "# Make sure x_test and y_test are defined in Section 2.\n",
    "\n",
    "# print(\"\\nEvaluating model on test data...\")\n",
    "# results = model.evaluate(x_test, y_test, batch_size=128)\n",
    "# print(f\"Test loss: {results[0]:.4f}\")\n",
    "# print(f\"Test accuracy: {results[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Make Predictions\n",
    "\n",
    "Use the trained model to make predictions on new, unseen images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for making predictions\n",
    "# You would typically load new images, preprocess them similarly to the training data,\n",
    "# and then use model.predict()\n",
    "\n",
    "# Example:\n",
    "# new_images = ... # Load and preprocess new images\n",
    "# predictions = model.predict(new_images)\n",
    "# predicted_classes = np.argmax(predictions, axis=1)\n",
    "# print(f\"Predictions: {predictions}\")\n",
    "# print(f\"Predicted classes: {predicted_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Steps & Considerations:\n",
    "\n",
    "- **Data Augmentation:** Increase the diversity of your training data by applying random transformations (rotations, flips, zooms) to your images. Keras has `ImageDataGenerator` or `tf.image` functions for this.\n",
    "- **Transfer Learning:** Use a pre-trained model (e.g., VGG16, ResNet, MobileNet) as a feature extractor or fine-tune it on your Galaxy Zoo data. This can be very effective, especially with smaller datasets.\n",
    "- **Hyperparameter Tuning:** Experiment with different learning rates, batch sizes, number of layers, filters, etc., to find the optimal configuration.\n",
    "- **Regularization:** Techniques like L1/L2 regularization or more dropout can help prevent overfitting.\n",
    "- **Callbacks:** Use Keras callbacks for tasks like saving the best model during training (`ModelCheckpoint`), early stopping (`EarlyStopping`), or adjusting the learning rate (`ReduceLROnPlateau`).\n",
    "- **More Complex Architectures:** Explore more advanced CNN architectures if needed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
