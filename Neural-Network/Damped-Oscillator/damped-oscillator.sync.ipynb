{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f47b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1. Daten vorbereiten ---\n",
    "def generate_damped_oscillator_data(num_samples=1000,\n",
    "                                   amplitude=1.0,\n",
    "                                   decay_constant=0.5,\n",
    "                                   frequency=2.0,\n",
    "                                   phase=0.0,\n",
    "                                   noise_amplitude=0.05):\n",
    "    \"\"\"\n",
    "    Generiert synthetische Daten für einen gedämpften harmonischen Oszillator.\n",
    "\n",
    "    Args:\n",
    "        num_samples (int): Anzahl der zu generierenden Datenpunkte.\n",
    "        amplitude (float): Anfangsamplitude des Oszillators.\n",
    "        decay_constant (float): Die Abklingkonstante (Gamma).\n",
    "        frequency (float): Die Winkelfrequenz (Omega).\n",
    "        phase (float): Die Phasenverschiebung.\n",
    "        noise_amplitude (float): Amplitude des hinzugefügten Rauschens.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Ein Tupel von (time_steps, positions)\n",
    "    \"\"\"\n",
    "    time_steps = np.linspace(0, 10, num_samples) # Zeit von 0 bis 10\n",
    "    true_positions = amplitude * np.exp(-decay_constant * time_steps) * \\\n",
    "                     np.cos(frequency * time_steps + phase)\n",
    "    # Rauschen hinzufügen, um das Modell robuster zu machen\n",
    "    noise = noise_amplitude * np.random.randn(num_samples)\n",
    "    positions = true_positions + noise\n",
    "    return time_steps, positions\n",
    "\n",
    "# Generieren der Daten\n",
    "time_data, position_data = generate_damped_oscillator_data()\n",
    "\n",
    "# Daten für Keras vorbereiten (Input muss 2D sein, Output kann 1D sein)\n",
    "# reshape(-1, 1) stellt sicher, dass es eine Spalte und beliebig viele Zeilen hat\n",
    "X = time_data.reshape(-1, 1)\n",
    "y = position_data\n",
    "\n",
    "# Daten aufteilen in Trainings- und Testsets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Shape von X_train: {X_train.shape}\")\n",
    "print(f\"Shape von y_train: {y_train.shape}\")\n",
    "print(f\"Shape von X_test: {X_test.shape}\")\n",
    "print(f\"Shape von y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a314af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Modell erstellen (Anzahl der Schichten und Neuronen) ---\n",
    "\n",
    "def build_oscillator_model(num_hidden_layers=1,\n",
    "                           neurons_per_layer=32,\n",
    "                           activation_function='relu',\n",
    "                           learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Erstellt ein Keras-Modell für die Vorhersage eines gedämpften Oszillators.\n",
    "\n",
    "    Args:\n",
    "        num_hidden_layers (int): Anzahl der versteckten Schichten.\n",
    "        neurons_per_layer (int): Anzahl der Neuronen in jeder versteckten Schicht.\n",
    "        activation_function (str): Name der Aktivierungsfunktion für die versteckten Schichten.\n",
    "        learning_rate (float): Die Lernrate für den Adam-Optimierer.\n",
    "\n",
    "    Returns:\n",
    "        keras.Model: Das kompilierte Keras-Modell.\n",
    "    \"\"\"\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # Eingabeschicht (implizit, durch die erste Dense-Schicht definiert)\n",
    "    # Input-Shape ist (1,), da wir nur die Zeit als Eingabe haben\n",
    "    model.add(layers.Dense(neurons_per_layer, input_shape=(1,), activation=activation_function))\n",
    "\n",
    "    # Versteckte Schichten\n",
    "    for _ in range(num_hidden_layers -1): # Beginnen bei 1, da die erste Schicht bereits hinzugefügt wurde\n",
    "        model.add(layers.Dense(neurons_per_layer, activation=activation_function))\n",
    "\n",
    "    # Ausgabeschicht\n",
    "    # Da wir eine einzelne kontinuierliche Position vorhersagen, ist die Ausgabe ein einzelnes Neuron\n",
    "    # und keine Aktivierungsfunktion (oder 'linear'), da wir keine Begrenzung der Ausgabe wünschen.\n",
    "    model.add(layers.Dense(1, activation='linear')) # 'linear' ist die Standardaktivierung und kann weggelassen werden\n",
    "\n",
    "    # --- 3. Optimierer konfigurieren ---\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    # --- 4. Modell kompilieren ---\n",
    "    # Metrik: Mean Absolute Error (MAE) ist gut, um die durchschnittliche absolute Abweichung zu sehen.\n",
    "    # Mean Squared Error (MSE) ist ebenfalls üblich und sensitiver auf größere Fehler.\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# --- Konfiguration der Modellparameter (für Einfachheit und Präzision) ---\n",
    "# Für \"einfachst\" und \"höchste Präzision\" müssen wir ein wenig experimentieren.\n",
    "# Beginnen wir mit einem sehr einfachen Setup und passen es bei Bedarf an.\n",
    "\n",
    "# Parameter für die Modellarchitektur\n",
    "NUM_HIDDEN_LAYERS = 2     # Eine oder zwei versteckte Schichten sind oft ein guter Startpunkt.\n",
    "                           # Für \"einfachst\" könnten wir sogar 1 versuchen.\n",
    "NEURONS_PER_LAYER = 32     # Eine kleinere Anzahl von Neuronen (z.B. 16, 32, 64)\n",
    "ACTIVATION_FUNCTION = 'relu' # ReLU ist eine gute Standardwahl. Auch 'tanh' könnte funktionieren.\n",
    "LEARNING_RATE = 0.001      # Eine typische Lernrate für Adam.\n",
    "\n",
    "# Erstelle das Modell\n",
    "model = build_oscillator_model(num_hidden_layers=NUM_HIDDEN_LAYERS,\n",
    "                               neurons_per_layer=NEURONS_PER_LAYER,\n",
    "                               activation_function=ACTIVATION_FUNCTION,\n",
    "                               learning_rate=LEARNING_RATE)\n",
    "\n",
    "# Zeige eine Zusammenfassung des Modells (Parameteranzahl ist hier sichtbar)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b010fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Modell trainieren ---\n",
    "# Das Training kann eine Weile dauern, abhängig von der Datenmenge und der Modellkomplexität.\n",
    "# 'epochs' ist die Anzahl der Trainingsdurchläufe über den gesamten Datensatz.\n",
    "# 'batch_size' ist die Anzahl der Samples, die pro Aktualisierung der Modellgewichte verwendet werden.\n",
    "# 'validation_split' reserviert einen Teil der Trainingsdaten für die Validierung während des Trainings.\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=200,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2,\n",
    "                    verbose=10) # verbose=1 zeigt den Trainingsfortschritt an\n",
    "\n",
    "# --- 6. Modell evaluieren ---\n",
    "loss, mae = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\nVerlust (MSE) auf dem Testset: {loss:.4f}\")\n",
    "print(f\"Mittlere absolute Abweichung (MAE) auf dem Testset: {mae:.4f}\")\n",
    "\n",
    "# --- Vorhersagen machen und visualisieren ---\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(X, y, label='Echte Daten (mit Rauschen)', alpha=0.6, s=10)\n",
    "plt.plot(X, y_pred, color='red', label='Modellvorhersage', linewidth=2)\n",
    "plt.title('Vorhersage eines gedämpften harmonischen Oszillators')\n",
    "plt.xlabel('Zeit')\n",
    "plt.ylabel('Position')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Optional: Trainingsverlauf plotten\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Trainings-Verlust')\n",
    "plt.plot(history.history['val_loss'], label='Validierungs-Verlust')\n",
    "plt.title('Modell-Verlust während des Trainings')\n",
    "plt.xlabel('Epoche')\n",
    "plt.ylabel('Verlust (MSE)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['mae'], label='Trainings-MAE')\n",
    "plt.plot(history.history['val_mae'], label='Validierungs-MAE')\n",
    "plt.title('Modell-MAE während des Trainings')\n",
    "plt.xlabel('Epoche')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
