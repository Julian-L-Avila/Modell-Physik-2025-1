{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "id": "RUACV5FKR8h6",
        "outputId": "dee79f47-8bdd-4d53-d6f3-4ad51693b747"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape von X_train: (800, 1)\n",
            "Shape von y_train: (800,)\n",
            "Shape von X_test: (200, 1)\n",
            "Shape von y_test: (200,)\n",
            "Building and training model: 32RELU32RELU...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-41735b550e1e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0;31m# Ensure X_train, y_train, X_test, y_test are available from the data prep step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m             history = model.fit(\n\u001b[0m\u001b[1;32m    281\u001b[0m                 \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m                 \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# As per user's original code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    393\u001b[0m                         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                     )\n\u001b[0;32m--> 395\u001b[0;31m                 val_logs = self.evaluate(\n\u001b[0m\u001b[1;32m    396\u001b[0m                     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m                     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m                 \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epoch_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py\u001b[0m in \u001b[0;36m_enumerate_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_seen\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_seen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m       raise RuntimeError(\"`tf.data.Dataset` only supports Python-style \"\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    707\u001b[0m             \u001b[0;34m\"When `dataset` is provided, `element_spec` and `components` must \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m             \"not be specified.\")\n\u001b[0;32m--> 709\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_next_call_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    746\u001b[0m             self._flat_output_types)\n\u001b[1;32m    747\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_set_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfulltype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3476\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3477\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3478\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   3479\u001b[0m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[1;32m   3480\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# ---\n",
        "# jupyter:\n",
        "#   jupytext:\n",
        "#     text_representation:\n",
        "#       extension: .py\n",
        "#       format_name: percent\n",
        "#       format_version: '1.3'\n",
        "#       jupytext_version: 1.3.4\n",
        "#   kernelspec:\n",
        "#     display_name: Python 3\n",
        "#     language: python\n",
        "#     name: python3\n",
        "# ---\n",
        "\n",
        "# %%\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "# --- Custom Activation Functions ---\n",
        "def custom_sechlu(x, rho=1.0):\n",
        "    \"\"\"\n",
        "    Implementiert eine Sigmoid-weighted Linear Unit basierend auf der Formel:\n",
        "    f(x) = x / (1 + exp(-2x / rho))\n",
        "\n",
        "    Diese Funktion skaliert die Eingabe x mit einer Sigmoid-ähnlichen Kurve.\n",
        "    Der Parameter rho steuert die Steilheit der Kurve um den Ursprung.\n",
        "\n",
        "    Argumente:\n",
        "        x (tf.Tensor): Der Eingabe-Tensor.\n",
        "        rho (float): Ein anpassbarer Parameter zur Steuerung der Form.\n",
        "                     Standardwert ist 1.0.\n",
        "\n",
        "    Rückgabe:\n",
        "        tf.Tensor: Der Tensor nach Anwendung der Aktivierungsfunktion.\n",
        "    \"\"\"\n",
        "    # Die Formel kann direkt mit TensorFlow-Operationen umgesetzt werden.\n",
        "    # tf.sigmoid(y) ist mathematisch äquivalent zu 1 / (1 + exp(-y)).\n",
        "    # Hier ist y = 2x / rho.\n",
        "    return x * tf.sigmoid(2 * x / rho)\n",
        "\n",
        "def custom_cauchylu(x, rho=1.0):\n",
        "    \"\"\"\n",
        "    Implementiert eine Cauchy-basierte Aktivierungsfunktion mit der Formel:\n",
        "    f(x) = x/2 * (1 + 2/pi * arctan(x/rho))\n",
        "\n",
        "    Diese Funktion skaliert die Eingabe auf eine komplexe, nicht-lineare Weise.\n",
        "    Der Parameter rho passt die Breite der zentralen Region an.\n",
        "\n",
        "    Argumente:\n",
        "        x (tf.Tensor): Der Eingabe-Tensor.\n",
        "        rho (float): Ein anpassbarer Parameter zur Steuerung der Form.\n",
        "                     Standardwert ist 1.0.\n",
        "\n",
        "    Rückgabe:\n",
        "        tf.Tensor: Der Tensor nach Anwendung der Aktivierungsfunktion.\n",
        "    \"\"\"\n",
        "    # Wir setzen die Formel Schritt für Schritt um\n",
        "\n",
        "    # 1. Berechne den inneren Teil: x / rho\n",
        "    scaled_x = x / rho\n",
        "\n",
        "    # 2. Berechne den Arcustangens\n",
        "    arctan_x = tf.math.atan(scaled_x)\n",
        "\n",
        "    # 3. Berechne den gesamten Skalierungsfaktor\n",
        "    gating_factor = 0.5 * (1.0 + (2.0 / np.pi) * arctan_x)\n",
        "\n",
        "    # 4. Multipliziere mit der ursprünglichen Eingabe x\n",
        "    #    (Hinweis: Die Formel wurde leicht umgestellt von x/2 * (...) zu x * (0.5 * (...))\n",
        "    #    für eine klarere Implementierung, das Ergebnis ist identisch)\n",
        "    return x * gating_factor\n",
        "\n",
        "def custom_laplacelu(x, rho=1.0):\n",
        "    \"\"\"\n",
        "    Implementiert eine Laplace-basierte Aktivierungsfunktion mit der Formel:\n",
        "    f(x, rho) = x/2 * (1 + sgn(x) * (1 - exp(-abs(x)/rho)))\n",
        "\n",
        "    Diese Funktion skaliert die Eingabe x mit einem Faktor, der von der\n",
        "    kumulativen Verteilungsfunktion der Laplace-Verteilung inspiriert ist.\n",
        "    Der Parameter rho steuert die Steilheit der Kurve.\n",
        "\n",
        "    Argumente:\n",
        "        x (tf.Tensor): Der Eingabe-Tensor.\n",
        "        rho (float): Ein anpassbarer Parameter zur Steuerung der Form.\n",
        "                     Standardwert ist 1.0.\n",
        "\n",
        "    Rückgabe:\n",
        "        tf.Tensor: Der Tensor nach Anwendung der Aktivierungsfunktion.\n",
        "    \"\"\"\n",
        "    # Wir setzen die Formel direkt mit TensorFlow-Funktionen um.\n",
        "\n",
        "    # 1. Berechne den Absolutwert von x\n",
        "    abs_x = tf.abs(x)\n",
        "\n",
        "    # 2. Berechne den exponentiellen Term\n",
        "    exp_term = tf.exp(-abs_x / rho)\n",
        "\n",
        "    # 3. Berechne den von der Vorzeichenfunktion abhängigen Teil\n",
        "    sign_term = tf.sign(x) * (1.0 - exp_term)\n",
        "\n",
        "    # 4. Berechne den gesamten Skalierungsfaktor (\"Gate\")\n",
        "    gating_factor = 0.5 * (1.0 + sign_term)\n",
        "\n",
        "    # 5. Multipliziere mit der ursprünglichen Eingabe x\n",
        "    #    (Umstellung von x/2 * (...) zu x * (0.5 * (...)) für Klarheit)\n",
        "    return x * gating_factor\n",
        "\n",
        "# It's also good practice to register them with Keras if they are going to be used by string name directly in layers\n",
        "# However, for this task, we will pass the function objects directly.\n",
        "\n",
        "# --- 1. Daten vorbereiten ---\n",
        "def generate_damped_oscillator_data(num_samples=1000,\n",
        "                                   amplitude=1.0,\n",
        "                                   decay_constant=0.5,\n",
        "                                   frequency=2.0,\n",
        "                                   phase=0.0,\n",
        "                                   noise_amplitude=0.00):\n",
        "    \"\"\"\n",
        "    Generiert synthetische Daten für einen gedämpften harmonischen Oszillator.\n",
        "\n",
        "    Args:\n",
        "        num_samples (int): Anzahl der zu generierenden Datenpunkte.\n",
        "        amplitude (float): Anfangsamplitude des Oszillators.\n",
        "        decay_constant (float): Die Abklingkonstante (Gamma).\n",
        "        frequency (float): Die Winkelfrequenz (Omega).\n",
        "        phase (float): Die Phasenverschiebung.\n",
        "        noise_amplitude (float): Amplitude des hinzugefügten Rauschens.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Ein Tupel von (time_steps, positions)\n",
        "    \"\"\"\n",
        "    time_steps = np.linspace(0, 10, num_samples) # Zeit von 0 bis 10\n",
        "    true_positions = amplitude * np.exp(-decay_constant * time_steps) * \\\n",
        "                     np.cos(frequency * time_steps + phase)\n",
        "    # Rauschen hinzufügen, um das Modell robuster zu machen\n",
        "    noise = noise_amplitude * np.random.randn(num_samples)\n",
        "    positions = true_positions + noise\n",
        "    return time_steps, positions\n",
        "\n",
        "# Generieren der Daten\n",
        "time_data, position_data = generate_damped_oscillator_data()\n",
        "\n",
        "# Daten für Keras vorbereiten (Input muss 2D sein, Output kann 1D sein)\n",
        "# reshape(-1, 1) stellt sicher, dass es eine Spalte und beliebig viele Zeilen hat\n",
        "X = time_data.reshape(-1, 1)\n",
        "y = position_data\n",
        "\n",
        "# Daten aufteilen in Trainings- und Testsets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Shape von X_train: {X_train.shape}\")\n",
        "print(f\"Shape von y_train: {y_train.shape}\")\n",
        "print(f\"Shape von X_test: {X_test.shape}\")\n",
        "print(f\"Shape von y_test: {y_test.shape}\")\n",
        "\n",
        "# %%\n",
        "# --- Main Experiment Loop ---\n",
        "neuron_configurations = [[32, 32], [32, 16], [16, 32]]\n",
        "# Custom activation functions (custom_sechlu, custom_cauchylu, custom_laplacelu) are already defined globally.\n",
        "\n",
        "activation_function_map = {\n",
        "    'relu': tf.keras.activations.relu,\n",
        "    'gelu': tf.keras.activations.gelu,\n",
        "    'tanh': tf.keras.activations.tanh,\n",
        "    # mish requires TensorFlow >= 2.3 (tf.keras.activations.mish) or from tf_addons.\n",
        "    # Assuming the execution environment supports tf.keras.activations.mish.\n",
        "    'mish': tf.keras.activations.mish,\n",
        "    'sechlu': custom_sechlu,\n",
        "    'cauchylu': custom_cauchylu,\n",
        "    'laplacelu': custom_laplacelu\n",
        "}\n",
        "activation_function_names = list(activation_function_map.keys())\n",
        "\n",
        "results_list = []\n",
        "\n",
        "# --- 2. Modell erstellen (Anzahl der Schichten und Neuronen) ---\n",
        "# Note: build_oscillator_model is defined below this block, which is fine.\n",
        "\n",
        "def build_oscillator_model(layer1_neurons,\n",
        "                           layer1_activation_func,\n",
        "                           layer2_neurons,\n",
        "                           layer2_activation_func,\n",
        "                           learning_rate=0.001,\n",
        "                           input_shape=(1,)):\n",
        "    \"\"\"\n",
        "    Erstellt ein Keras-Modell für die Vorhersage eines gedämpften Oszillators.\n",
        "\n",
        "    Args:\n",
        "        layer1_neurons (int): Anzahl der Neuronen in der ersten Schicht.\n",
        "        layer1_activation_func (callable or str): Aktivierungsfunktion für die erste Schicht.\n",
        "        layer2_neurons (int): Anzahl der Neuronen in der zweiten Schicht.\n",
        "        layer2_activation_func (callable or str): Aktivierungsfunktion für die zweite Schicht.\n",
        "        learning_rate (float): Die Lernrate für den Adam-Optimierer.\n",
        "        input_shape (tuple): Input-Shape für die erste Schicht.\n",
        "\n",
        "    Returns:\n",
        "        keras.Model: Das kompilierte Keras-Modell.\n",
        "    \"\"\"\n",
        "    model = keras.Sequential()\n",
        "\n",
        "    # Erste Dense Schicht\n",
        "    model.add(layers.Dense(layer1_neurons, activation=layer1_activation_func, input_shape=input_shape))\n",
        "\n",
        "    # Zweite Dense Schicht\n",
        "    model.add(layers.Dense(layer2_neurons, activation=layer2_activation_func))\n",
        "\n",
        "    # Ausgabeschicht\n",
        "    model.add(layers.Dense(1, activation='linear'))\n",
        "\n",
        "    # --- 3. Optimierer konfigurieren ---\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "    # --- 4. Modell kompilieren ---\n",
        "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# --- Konfiguration der Modellparameter (für Einfachheit und Präzision) ---\n",
        "# Für \"einfachst\" und \"höchste Präzision\" müssen wir ein wenig experimentieren.\n",
        "# Beginnen wir mit einem sehr einfachen Setup und passen es bei Bedarf an.\n",
        "\n",
        "# Parameter für die Modellarchitektur\n",
        "# Diese Werte werden nun direkt beim Aufruf von build_oscillator_model übergeben.\n",
        "# Beispiel:\n",
        "# model = build_oscillator_model(layer1_neurons=32, layer1_activation_func='relu',\n",
        "#                                layer2_neurons=32, layer2_activation_func='relu',\n",
        "#                                learning_rate=0.001)\n",
        "\n",
        "\n",
        "# Erstelle das Modell - Beispielaufruf (muss angepasst werden, falls Training direkt hier erfolgen soll)\n",
        "# Die alten Parameter NUM_HIDDEN_LAYERS, NEURONS_PER_LAYER, ACTIVATION_FUNCTION, LEARNING_RATE\n",
        "# sind nicht mehr direkt für build_oscillator_model relevant in dieser Form.\n",
        "# Stattdessen werden die Parameter direkt übergeben.\n",
        "# Für den Moment kommentieren wir die Modellerstellung und model.summary() aus,\n",
        "# da die Parameter nicht mehr auf die alte Weise definiert sind.\n",
        "# model = build_oscillator_model(num_hidden_layers=NUM_HIDDEN_LAYERS,\n",
        "#                                neurons_per_layer=NEURONS_PER_LAYER,\n",
        "#                                activation_function=ACTIVATION_FUNCTION,\n",
        "#                                learning_rate=LEARNING_RATE)\n",
        "\n",
        "# Zeige eine Zusammenfassung des Modells (Parameteranzahl ist hier sichtbar)\n",
        "# model.summary()\n",
        "\n",
        "# %%\n",
        "# --- Main Experiment Loop (continued) ---\n",
        "for neurons_config in neuron_configurations:\n",
        "    layer1_neurons, layer2_neurons = neurons_config\n",
        "    for act_name1 in activation_function_names:\n",
        "        for act_name2 in activation_function_names:\n",
        "            act_func1 = activation_function_map[act_name1]\n",
        "            act_func2 = activation_function_map[act_name2]\n",
        "\n",
        "            model_name = f\"{layer1_neurons}{act_name1.upper()}{layer2_neurons}{act_name2.upper()}\"\n",
        "\n",
        "            print(f\"Building and training model: {model_name}...\")\n",
        "\n",
        "            model = build_oscillator_model(\n",
        "                layer1_neurons=layer1_neurons,\n",
        "                layer1_activation_func=act_func1,\n",
        "                layer2_neurons=layer2_neurons,\n",
        "                layer2_activation_func=act_func2\n",
        "            )\n",
        "\n",
        "            num_params = model.count_params()\n",
        "\n",
        "            early_stopping_callback = EarlyStopping(\n",
        "                monitor='val_loss',\n",
        "                patience=50,  # As per user's original code\n",
        "                restore_best_weights=True\n",
        "            )\n",
        "\n",
        "            # Ensure X_train, y_train, X_test, y_test are available from the data prep step\n",
        "            history = model.fit(\n",
        "                X_train, y_train,\n",
        "                epochs=2000, # As per user's original code\n",
        "                validation_data=(X_test, y_test),\n",
        "                batch_size=32,\n",
        "                callbacks=[early_stopping_callback],\n",
        "                verbose=0  # Keep output clean during the loop\n",
        "            )\n",
        "\n",
        "            actual_epochs = len(history.history['loss'])\n",
        "\n",
        "            # Evaluate the model on the test set\n",
        "            loss, mae = model.evaluate(X_test, y_test, verbose=0)\n",
        "            mse = loss  # model.compile uses 'mean_squared_error' as the loss (already updated)\n",
        "\n",
        "            results_list.append([model_name, num_params, actual_epochs, mae, mse])\n",
        "            print(f\"Completed: {model_name} | Params: {num_params} | Epochs: {actual_epochs} | MAE: {mae:.4f} | MSE: {mse:.4f}\")\n",
        "\n",
        "            # Optional: Clear session to free memory if many models are trained\n",
        "            # tf.keras.backend.clear_session()\n",
        "            # Note: clear_session() also clears custom object registry, so use with care if custom objects are not re-registered.\n",
        "            # For this case, it might be okay as models are built fresh each time.\n",
        "            # Let's not add clear_session() for now to avoid potential complexities unless memory issues arise.\n",
        "\n",
        "# --- Save results to TSV ---\n",
        "output_tsv_file = 'model_comparison_results.tsv'\n",
        "print(f\"Writing results to {output_tsv_file}...\")\n",
        "\n",
        "with open(output_tsv_file, 'w', newline='') as tsvfile:\n",
        "    writer = csv.writer(tsvfile, delimiter='\\t') # Using tab as delimiter\n",
        "\n",
        "    # Write the header\n",
        "    writer.writerow(['Name of model', 'parameters', 'epochs', 'mae', 'mse'])\n",
        "\n",
        "    # Write the data rows\n",
        "    for row in results_list: # Assuming results_list is populated by the experiment loop\n",
        "        writer.writerow(row)\n",
        "\n",
        "print(f\"Results successfully saved to {output_tsv_file}.\")\n",
        "\n",
        "# %%\n",
        "# --- (Old sections for single model training, evaluation, and plotting are now replaced by the loop above) ---\n",
        "# --- 5. Modell trainieren ---\n",
        "# ... (code removed / commented out) ...\n",
        "#\n",
        "# --- 6. Modell evaluieren ---\n",
        "# ... (code removed / commented out) ...\n",
        "#\n",
        "# --- Vorhersagen machen und visualisieren ---\n",
        "# ... (code removed / commented out) ...\n",
        "#\n",
        "# Optional: Trainingsverlauf plotten\n",
        "# ... (code removed / commented out) ..."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building and training model: 32RELU32RELU...\n",
        "Completed: 32RELU32RELU | Params: 1153 | Epochs: 70 | MAE: 0.0896 | MSE: 0.0172\n",
        "Building and training model: 32RELU32GELU...\n",
        "Completed: 32RELU32GELU | Params: 1153 | Epochs: 678 | MAE: 0.0042 | MSE: 0.0000\n",
        "Building and training model: 32RELU32TANH...\n",
        "Completed: 32RELU32TANH | Params: 1153 | Epochs: 974 | MAE: 0.0025 | MSE: 0.0000\n",
        "Building and training model: 32RELU32MISH...\n",
        "Completed: 32RELU32MISH | Params: 1153 | Epochs: 1521 | MAE: 0.0015 | MSE: 0.0000\n",
        "Building and training model: 32RELU32SECHLU...\n",
        "Completed: 32RELU32SECHLU | Params: 1153 | Epochs: 1136 | MAE: 0.0041 | MSE: 0.0000\n",
        "Building and training model: 32RELU32CAUCHYLU...\n",
        "Completed: 32RELU32CAUCHYLU | Params: 1153 | Epochs: 999 | MAE: 0.0033 | MSE: 0.0000\n",
        "Building and training model: 32RELU32LAPLACELU...\n",
        "Completed: 32RELU32LAPLACELU | Params: 1153 | Epochs: 608 | MAE: 0.0060 | MSE: 0.0001\n",
        "Building and training model: 32GELU32RELU...\n",
        "Completed: 32GELU32RELU | Params: 1153 | Epochs: 1900 | MAE: 0.0032 | MSE: 0.0000\n",
        "Building and training model: 32GELU32GELU...\n",
        "Completed: 32GELU32GELU | Params: 1153 | Epochs: 869 | MAE: 0.0023 | MSE: 0.0000\n",
        "Building and training model: 32GELU32TANH...\n",
        "Completed: 32GELU32TANH | Params: 1153 | Epochs: 938 | MAE: 0.0034 | MSE: 0.0000\n",
        "Building and training model: 32GELU32MISH...\n",
        "Completed: 32GELU32MISH | Params: 1153 | Epochs: 954 | MAE: 0.0056 | MSE: 0.0001\n",
        "Building and training model: 32GELU32SECHLU...\n",
        "Completed: 32GELU32SECHLU | Params: 1153 | Epochs: 711 | MAE: 0.0043 | MSE: 0.0000\n",
        "Building and training model: 32GELU32CAUCHYLU...\n",
        "Completed: 32GELU32CAUCHYLU | Params: 1153 | Epochs: 1403 | MAE: 0.0047 | MSE: 0.0000\n",
        "Building and training model: 32GELU32LAPLACELU...\n",
        "Completed: 32GELU32LAPLACELU | Params: 1153 | Epochs: 1138 | MAE: 0.0048 | MSE: 0.0000\n",
        "Building and training model: 32TANH32RELU...\n",
        "Completed: 32TANH32RELU | Params: 1153 | Epochs: 1032 | MAE: 0.0057 | MSE: 0.0001\n",
        "Building and training model: 32TANH32GELU...\n",
        "Completed: 32TANH32GELU | Params: 1153 | Epochs: 904 | MAE: 0.0041 | MSE: 0.0000\n",
        "Building and training model: 32TANH32TANH...\n",
        "Completed: 32TANH32TANH | Params: 1153 | Epochs: 1043 | MAE: 0.0040 | MSE: 0.0000\n",
        "Building and training model: 32TANH32MISH...\n",
        "Completed: 32TANH32MISH | Params: 1153 | Epochs: 992 | MAE: 0.0059 | MSE: 0.0001\n",
        "Building and training model: 32TANH32SECHLU...\n",
        "Completed: 32TANH32SECHLU | Params: 1153 | Epochs: 581 | MAE: 0.0045 | MSE: 0.0000\n",
        "Building and training model: 32TANH32CAUCHYLU...\n",
        "Completed: 32TANH32CAUCHYLU | Params: 1153 | Epochs: 930 | MAE: 0.0038 | MSE: 0.0000\n",
        "Building and training model: 32TANH32LAPLACELU...\n",
        "Completed: 32TANH32LAPLACELU | Params: 1153 | Epochs: 1480 | MAE: 0.0045 | MSE: 0.0000\n",
        "Building and training model: 32MISH32RELU...\n",
        "Completed: 32MISH32RELU | Params: 1153 | Epochs: 372 | MAE: 0.0066 | MSE: 0.0001\n",
        "Building and training model: 32MISH32GELU...\n",
        "Completed: 32MISH32GELU | Params: 1153 | Epochs: 822 | MAE: 0.0030 | MSE: 0.0000\n",
        "Building and training model: 32MISH32TANH...\n",
        "Completed: 32MISH32TANH | Params: 1153 | Epochs: 735 | MAE: 0.0043 | MSE: 0.0000\n",
        "Building and training model: 32MISH32MISH...\n",
        "Completed: 32MISH32MISH | Params: 1153 | Epochs: 879 | MAE: 0.0045 | MSE: 0.0000\n",
        "Building and training model: 32MISH32SECHLU...\n",
        "Completed: 32MISH32SECHLU | Params: 1153 | Epochs: 479 | MAE: 0.0041 | MSE: 0.0000\n",
        "Building and training model: 32MISH32CAUCHYLU...\n",
        "Completed: 32MISH32CAUCHYLU | Params: 1153 | Epochs: 766 | MAE: 0.0054 | MSE: 0.0000\n",
        "Building and training model: 32MISH32LAPLACELU...\n",
        "Completed: 32MISH32LAPLACELU | Params: 1153 | Epochs: 1363 | MAE: 0.0010 | MSE: 0.0000\n",
        "Building and training model: 32SECHLU32RELU...\n",
        "Completed: 32SECHLU32RELU | Params: 1153 | Epochs: 796 | MAE: 0.0051 | MSE: 0.0001\n",
        "Building and training model: 32SECHLU32GELU...\n",
        "Completed: 32SECHLU32GELU | Params: 1153 | Epochs: 606 | MAE: 0.0057 | MSE: 0.0001\n",
        "Building and training model: 32SECHLU32TANH...\n",
        "Completed: 32SECHLU32TANH | Params: 1153 | Epochs: 929 | MAE: 0.0023 | MSE: 0.0000\n",
        "Building and training model: 32SECHLU32MISH...\n",
        "Completed: 32SECHLU32MISH | Params: 1153 | Epochs: 1196 | MAE: 0.0048 | MSE: 0.0000\n",
        "Building and training model: 32SECHLU32SECHLU...\n",
        "Completed: 32SECHLU32SECHLU | Params: 1153 | Epochs: 824 | MAE: 0.0052 | MSE: 0.0000\n",
        "Building and training model: 32SECHLU32CAUCHYLU...\n",
        "Completed: 32SECHLU32CAUCHYLU | Params: 1153 | Epochs: 1001 | MAE: 0.0051 | MSE: 0.0000\n",
        "Building and training model: 32SECHLU32LAPLACELU...\n",
        "Completed: 32SECHLU32LAPLACELU | Params: 1153 | Epochs: 929 | MAE: 0.0081 | MSE: 0.0001\n",
        "Building and training model: 32CAUCHYLU32RELU...\n",
        "Completed: 32CAUCHYLU32RELU | Params: 1153 | Epochs: 1825 | MAE: 0.0051 | MSE: 0.0000\n",
        "Building and training model: 32CAUCHYLU32GELU...\n",
        "Completed: 32CAUCHYLU32GELU | Params: 1153 | Epochs: 1295 | MAE: 0.0024 | MSE: 0.0000\n",
        "Building and training model: 32CAUCHYLU32TANH...\n",
        "Completed: 32CAUCHYLU32TANH | Params: 1153 | Epochs: 1659 | MAE: 0.0011 | MSE: 0.0000\n",
        "Building and training model: 32CAUCHYLU32MISH...\n",
        "Completed: 32CAUCHYLU32MISH | Params: 1153 | Epochs: 589 | MAE: 0.0075 | MSE: 0.0001\n",
        "Building and training model: 32CAUCHYLU32SECHLU...\n",
        "Completed: 32CAUCHYLU32SECHLU | Params: 1153 | Epochs: 1181 | MAE: 0.0032 | MSE: 0.0000\n",
        "Building and training model: 32CAUCHYLU32CAUCHYLU...\n",
        "Completed: 32CAUCHYLU32CAUCHYLU | Params: 1153 | Epochs: 675 | MAE: 0.0041 | MSE: 0.0000\n",
        "Building and training model: 32CAUCHYLU32LAPLACELU...\n",
        "Completed: 32CAUCHYLU32LAPLACELU | Params: 1153 | Epochs: 1106 | MAE: 0.0047 | MSE: 0.0000\n",
        "Building and training model: 32LAPLACELU32RELU...\n",
        "Completed: 32LAPLACELU32RELU | Params: 1153 | Epochs: 517 | MAE: 0.0043 | MSE: 0.0000\n",
        "Building and training model: 32LAPLACELU32GELU...\n",
        "Completed: 32LAPLACELU32GELU | Params: 1153 | Epochs: 649 | MAE: 0.0026 | MSE: 0.0000\n",
        "Building and training model: 32LAPLACELU32TANH...\n",
        "Completed: 32LAPLACELU32TANH | Params: 1153 | Epochs: 1110 | MAE: 0.0024 | MSE: 0.0000\n",
        "Building and training model: 32LAPLACELU32MISH...\n",
        "Completed: 32LAPLACELU32MISH | Params: 1153 | Epochs: 1655 | MAE: 0.0019 | MSE: 0.0000\n",
        "Building and training model: 32LAPLACELU32SECHLU...\n",
        "Completed: 32LAPLACELU32SECHLU | Params: 1153 | Epochs: 566 | MAE: 0.0044 | MSE: 0.0000\n",
        "Building and training model: 32LAPLACELU32CAUCHYLU...\n",
        "Completed: 32LAPLACELU32CAUCHYLU | Params: 1153 | Epochs: 843 | MAE: 0.0045 | MSE: 0.0000\n",
        "Building and training model: 32LAPLACELU32LAPLACELU...\n",
        "Completed: 32LAPLACELU32LAPLACELU | Params: 1153 | Epochs: 994 | MAE: 0.0055 | MSE: 0.0000\n",
        "Building and training model: 32RELU16RELU...\n",
        "Completed: 32RELU16RELU | Params: 609 | Epochs: 151 | MAE: 0.0394 | MSE: 0.0038\n",
        "Building and training model: 32RELU16GELU...\n",
        "Completed: 32RELU16GELU | Params: 609 | Epochs: 961 | MAE: 0.0020 | MSE: 0.0000\n",
        "Building and training model: 32RELU16TANH...\n",
        "Completed: 32RELU16TANH | Params: 609 | Epochs: 914 | MAE: 0.0022 | MSE: 0.0000\n",
        "Building and training model: 32RELU16MISH...\n",
        "Completed: 32RELU16MISH | Params: 609 | Epochs: 1138 | MAE: 0.0051 | MSE: 0.0000\n",
        "Building and training model: 32RELU16SECHLU...\n",
        "Completed: 32RELU16SECHLU | Params: 609 | Epochs: 730 | MAE: 0.0053 | MSE: 0.0000\n",
        "Building and training model: 32RELU16CAUCHYLU...\n",
        "Completed: 32RELU16CAUCHYLU | Params: 609 | Epochs: 1198 | MAE: 0.0039 | MSE: 0.0000\n",
        "Building and training model: 32RELU16LAPLACELU...\n",
        "Completed: 32RELU16LAPLACELU | Params: 609 | Epochs: 1562 | MAE: 0.0015 | MSE: 0.0000\n",
        "Building and training model: 32GELU16RELU...\n",
        "Completed: 32GELU16RELU | Params: 609 | Epochs: 793 | MAE: 0.0081 | MSE: 0.0001\n",
        "Building and training model: 32GELU16GELU...\n",
        "Completed: 32GELU16GELU | Params: 609 | Epochs: 876 | MAE: 0.0027 | MSE: 0.0000\n",
        "Building and training model: 32GELU16TANH...\n",
        "Completed: 32GELU16TANH | Params: 609 | Epochs: 1082 | MAE: 0.0046 | MSE: 0.0000\n",
        "Building and training model: 32GELU16MISH...\n",
        "Completed: 32GELU16MISH | Params: 609 | Epochs: 1772 | MAE: 0.0044 | MSE: 0.0000\n",
        "Building and training model: 32GELU16SECHLU...\n",
        "Completed: 32GELU16SECHLU | Params: 609 | Epochs: 494 | MAE: 0.0054 | MSE: 0.0000\n",
        "Building and training model: 32GELU16CAUCHYLU...\n",
        "Completed: 32GELU16CAUCHYLU | Params: 609 | Epochs: 1372 | MAE: 0.0035 | MSE: 0.0000\n",
        "Building and training model: 32GELU16LAPLACELU...\n",
        "Completed: 32GELU16LAPLACELU | Params: 609 | Epochs: 1148 | MAE: 0.0062 | MSE: 0.0001\n",
        "Building and training model: 32TANH16RELU...\n",
        "Completed: 32TANH16RELU | Params: 609 | Epochs: 463 | MAE: 0.0154 | MSE: 0.0004\n",
        "Building and training model: 32TANH16GELU...\n",
        "Completed: 32TANH16GELU | Params: 609 | Epochs: 1010 | MAE: 0.0052 | MSE: 0.0000\n",
        "Building and training model: 32TANH16TANH...\n",
        "Completed: 32TANH16TANH | Params: 609 | Epochs: 1196 | MAE: 0.0040 | MSE: 0.0000\n",
        "Building and training model: 32TANH16MISH...\n",
        "Completed: 32TANH16MISH | Params: 609 | Epochs: 1213 | MAE: 0.0060 | MSE: 0.0001\n",
        "Building and training model: 32TANH16SECHLU...\n",
        "Completed: 32TANH16SECHLU | Params: 609 | Epochs: 718 | MAE: 0.0064 | MSE: 0.0001\n",
        "Building and training model: 32TANH16CAUCHYLU...\n",
        "Completed: 32TANH16CAUCHYLU | Params: 609 | Epochs: 721 | MAE: 0.0048 | MSE: 0.0000\n",
        "Building and training model: 32TANH16LAPLACELU...\n",
        "Completed: 32TANH16LAPLACELU | Params: 609 | Epochs: 943 | MAE: 0.0060 | MSE: 0.0001\n",
        "Building and training model: 32MISH16RELU...\n",
        "Completed: 32MISH16RELU | Params: 609 | Epochs: 672 | MAE: 0.0086 | MSE: 0.0001\n",
        "Building and training model: 32MISH16GELU...\n",
        "Completed: 32MISH16GELU | Params: 609 | Epochs: 802 | MAE: 0.0028 | MSE: 0.0000\n",
        "Building and training model: 32MISH16TANH...\n",
        "Completed: 32MISH16TANH | Params: 609 | Epochs: 917 | MAE: 0.0020 | MSE: 0.0000\n",
        "Building and training model: 32MISH16MISH...\n",
        "Completed: 32MISH16MISH | Params: 609 | Epochs: 906 | MAE: 0.0047 | MSE: 0.0000\n",
        "Building and training model: 32MISH16SECHLU...\n",
        "Completed: 32MISH16SECHLU | Params: 609 | Epochs: 869 | MAE: 0.0033 | MSE: 0.0000\n",
        "Building and training model: 32MISH16CAUCHYLU...\n",
        "Completed: 32MISH16CAUCHYLU | Params: 609 | Epochs: 879 | MAE: 0.0042 | MSE: 0.0000\n",
        "Building and training model: 32MISH16LAPLACELU...\n",
        "Completed: 32MISH16LAPLACELU | Params: 609 | Epochs: 1365 | MAE: 0.0029 | MSE: 0.0000\n",
        "Building and training model: 32SECHLU16RELU...\n",
        "Completed: 32SECHLU16RELU | Params: 609 | Epochs: 473 | MAE: 0.0089 | MSE: 0.0001\n",
        "Building and training model: 32SECHLU16GELU...\n"
      ],
      "metadata": {
        "id": "RiYTE8cfhvKV"
      }
    }
  ]
}